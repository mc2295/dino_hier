I20250303 18:15:41 834514 dinov2 config.py:59] git:
  sha: ffce7c8cb811a2f26f369f345c1ed9f6f6db8642, status: has uncommitted changes, branch: main

I20250303 18:15:41 834514 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250303 18:15:41 834514 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250303 18:15:41 834514 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblas: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250303 18:15:43 834514 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250303 18:15:43 834514 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250303 18:15:43 834514 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250303 18:15:44 834514 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250303 18:15:54 834514 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250303 18:15:59 834514 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250303 18:16:00 834514 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250303 18:16:01 834514 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 10:34:25 1648810 dinov2 config.py:59] git:
  sha: ffce7c8cb811a2f26f369f345c1ed9f6f6db8642, status: has uncommitted changes, branch: main

I20250304 10:34:25 1648810 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 10:34:25 1648810 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 10:34:25 1648810 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblas: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 10:34:27 1648810 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 10:34:27 1648810 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 10:34:28 1648810 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 10:34:28 1648810 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 10:34:39 1648810 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 10:34:44 1648810 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 10:34:44 1648810 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 10:34:45 1648810 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 11:30:02 4175113 dinov2 config.py:59] git:
  sha: ffce7c8cb811a2f26f369f345c1ed9f6f6db8642, status: has uncommitted changes, branch: main

I20250304 11:30:02 4175113 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 11:30:02 4175113 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 11:30:02 4175113 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblas: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 11:30:05 4175113 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 11:30:05 4175113 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 11:30:05 4175113 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 11:30:06 4175113 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 11:30:21 4175113 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 11:30:26 4175113 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 11:30:26 4175113 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 11:30:27 4175113 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 11:38:52 4191566 dinov2 config.py:59] git:
  sha: ffce7c8cb811a2f26f369f345c1ed9f6f6db8642, status: has uncommitted changes, branch: main

I20250304 11:38:52 4191566 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 11:38:52 4191566 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 11:38:52 4191566 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblas: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 11:38:54 4191566 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 11:38:54 4191566 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 11:38:54 4191566 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 11:38:54 4191566 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 11:39:05 4191566 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 11:39:09 4191566 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 11:39:10 4191566 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 11:39:10 4191566 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 12:18:46 1281769 dinov2 config.py:59] git:
  sha: ffce7c8cb811a2f26f369f345c1ed9f6f6db8642, status: has uncommitted changes, branch: main

I20250304 12:18:47 1281769 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 12:18:47 1281769 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 12:18:47 1281769 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblas: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 12:18:49 1281769 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 12:18:49 1281769 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 12:18:49 1281769 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 12:18:50 1281769 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 12:19:00 1281769 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 12:19:06 1281769 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 12:19:06 1281769 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 12:19:07 1281769 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 12:28:49 1284815 dinov2 config.py:59] git:
  sha: f0e80bc90da77a40fa9c45f312ec7d8265a54d16, status: clean, branch: main

I20250304 12:28:49 1284815 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 12:28:49 1284815 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 12:28:49 1284815 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblast: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 12:28:51 1284815 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 12:28:51 1284815 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 12:28:51 1284815 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 12:28:52 1284815 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 12:29:05 1284815 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 12:29:10 1284815 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 12:29:10 1284815 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 12:29:11 1284815 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 14:12:43 1358468 dinov2 config.py:59] git:
  sha: f0e80bc90da77a40fa9c45f312ec7d8265a54d16, status: has uncommitted changes, branch: main

I20250304 14:12:44 1358468 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 14:12:44 1358468 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 14:12:44 1358468 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 27
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 22
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblast: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 14:12:45 1358468 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 14:12:45 1358468 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 14:12:46 1358468 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 14:12:46 1358468 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 14:12:59 1358468 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 14:13:04 1358468 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 14:13:04 1358468 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 14:13:05 1358468 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 18:08:26 731963 dinov2 config.py:59] git:
  sha: f0e80bc90da77a40fa9c45f312ec7d8265a54d16, status: has uncommitted changes, branch: main

I20250304 18:08:26 731963 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 18:08:26 731963 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 18:08:26 731963 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 26
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 26
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblast: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 18:08:28 731963 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 18:08:28 731963 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 18:08:28 731963 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 18:08:29 731963 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 18:08:41 731963 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 18:08:46 731963 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 18:08:47 731963 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 18:08:47 731963 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:256] Student and Teacher are built: they are both dinov2_vitl14 network.
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:560] DISTRIBUTED FSDP -- preparing model for distributed training
W20250304 18:08:53 731963 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:437: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20250304 18:08:53 731963 dinov2 train.py:382] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (ibot_patch_loss): iBOTPatchLoss()
  (supervised_losses): HierarchicalCrossEntropyLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-23): 24 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=384, bias=True)
        )
        (last_layer): Linear(in_features=384, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (supervised_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): MLP(
        (net): Sequential(
          (0): Linear(in_features=1024, out_features=26, bias=True)
        )
        (last_linear): Linear(in_features=1024, out_features=26, bias=True)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-23): 24 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=384, bias=True)
        )
        (last_layer): Linear(in_features=384, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20250304 18:08:53 731963 dinov2 param_groups.py:54] chunked fsdp
I20250304 18:08:53 731963 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.12.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.13.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.14.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.15.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.16.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.17.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.18.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.19.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.20.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.21.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.22.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] blocks.23.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 18:08:53 731963 dinov2 param_groups.py:64] else code branch
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 18:08:53 731963 dinov2 param_groups.py:64] else code branch
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 18:08:53 731963 dinov2 param_groups.py:64] else code branch
I20250304 18:08:53 731963 dinov2 param_groups.py:87] net.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 18:08:53 731963 dinov2 param_groups.py:87] net.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 18:08:53 731963 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 18:08:53 731963 dinov2 train.py:122] Schedulers ready.
I20250304 18:08:53 731963 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250304 18:08:53 731963 dinov2 augmentations.py:34] ###################################
I20250304 18:08:53 731963 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250304 18:08:53 731963 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250304 18:08:53 731963 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250304 18:08:53 731963 dinov2 augmentations.py:38] local_crops_number: 1
I20250304 18:08:53 731963 dinov2 augmentations.py:39] global_crops_size: 224
I20250304 18:08:53 731963 dinov2 augmentations.py:40] local_crops_size: 98
I20250304 18:08:53 731963 dinov2 augmentations.py:41] ###################################
I20250304 18:08:53 731963 dinov2 augmentations.py:128] ###################################
I20250304 18:08:53 731963 dinov2 augmentations.py:129] Using data augmentation parameters:
I20250304 18:08:53 731963 dinov2 augmentations.py:130] img_size: 224
I20250304 18:08:53 731963 dinov2 augmentations.py:131] ###################################
I20250304 18:08:53 731963 dinov2 loaders.py:93] using dataset: "UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1"
I20250304 19:03:26 474390 dinov2 config.py:59] git:
  sha: f0e80bc90da77a40fa9c45f312ec7d8265a54d16, status: has uncommitted changes, branch: main

I20250304 19:03:26 474390 dinov2 config.py:60] alpha: 0.5
config_file: dinov2_hier/configs/train/custom.yaml
eval: 
eval_only: False
hier: True
no_resume: True
opts: ['True', 'train.output_dir=/ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier']
output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
version: 1
I20250304 19:03:26 474390 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 5e-05
I20250304 19:03:26 474390 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    supervised_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    domain_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 384
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.0
  smooth_rank_loss_weight: 0.0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /ictstr01/home/aih/manon.chossegros/MICCAI/models/dino_hier
  saveckp_freq: 20
  seed: 0
  num_workers: 25
  OFFICIAL_EPOCH_LENGTH: 500
  cache_dataset: true
  centering: sinkhorn_knopp
  dataset_path_sup: Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  dataset_path_unsup: UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1
  n_classes: 26
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0
  num_register_tokens: 0
student:
  arch: dinov2_vitl14
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 5.0e-05
  warmup_epochs: 20
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 1
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 1000
data_transform: default
supervised:
  loss_weight: 1.0
  n_classes: 26
n_levels: 4
label_dict:
  basophil: basophil
  eosinophil: eosinophil
  erythroblast: erythroblast
  lymphocyte_typical: lymphocyte_typical
  lymphocyte: lymphocyte_mature
  metamyelocyte: metamyelocyte
  monocyte: monocyte
  myeloblast: myeloblast
  myelocyte: myelocyte
  neutrophil_band: neutrophil_band
  neutrophil_segmented: neutrophil_segmented
  promyelocyte: promyelocyte
  lymphoblast: lymphoblast
  platelet: platelet
  BAS: basophil
  EBO: erythroblast
  EOS: eosinophil
  KSC: smudge_cell
  LYT: lymphocyte_typical
  MMZ: metamyelocyte
  MON: monocyte
  MYB: myelocyte
  MYO: myeloblast
  NGB: neutrophil_band
  NGS: neutrophil_segmented
  PMO: promyelocyte
  PMB: promyelocyte_bilobed
  MOB: monoblast
  LYA: lymphocyte_atypical
  01-NORMO: erythroblast
  1-Normo: erythroblast
  05-MONO: monoblast
  5-MONO: monoblast
  09-BASO: basophil
  11-STAB: neutrophil_band
  13-MYBL: myeloblast
  15-SEG: neutrophil_segmented
  17-Kernschatten: smudge_cell
  19-MYEL: myelocyte
  21-Haarzelle: hairy_cell
  10-EOS: eosinophil
  14-LYMPH-typ: lymphocyte_typical
  16-PLZ: plasma_cell
  18-PMYEL: promyelocyte
  20-Meta: metamyelocyte
  20-META: metamyelocyte
  9-BASO: basophil
  12-LYMPH-reaktiv: lymphocyte_reactive
  08-LYMPH-neo: lymphocyte_neoplastic
  04-LGL: lymphocyte_large_granular
  Eosinophil: eosinophil
  Lymphocyte: lymphocyte_mature
  Lymphocyte-1: lymphocyte_mature
  Monocyte: monocyte
  Neutrophil: neutrophil
  Basophil: basophil
  basophile: basophil
  eosinophile: eosinophil
  ABE: eosinophil_abnormal
  BLA: blast
  FGC: fagott_cell
  HAC: hairy_cell
  LYI: lymphocyte_immature
  PEB: proeryhtroblast
  PLM: plasma_cell
start_from_dinobloom: false
classes_to_int:
  basophil: 0
  eosinophil_abnormal: 1
  erythroblast: 2
  fagott_cell: 3
  hairy_cell: 4
  lymphoblast: 5
  lymphocyte_immature: 6
  lymphocyte_large_granular: 7
  lymphocyte_neoplastic: 8
  lymphocyte_reactive: 9
  lymphocyte_typical: 10
  metamyelocyte: 11
  monoblast: 12
  monocyte: 13
  myeloblast: 14
  myelocyte: 15
  neutrophil_band: 16
  neutrophil_segmented: 17
  plasma_cell: 18
  proeryhtroblast: 19
  promyelocyte: 20
  promyelocyte_bilobed: 21
  blast: 22
  eosinophil: 23
  lymphocyte_atypical: 24
  lymphocyte_mature: 25
  neutrophil: 26
  platelet: 27
  smudge_cell: 28
'True': null

W20250304 19:03:28 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")

W20250304 19:03:28 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")

W20250304 19:03:28 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")

I20250304 19:03:29 474390 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 19:03:54 474390 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:138] OPTIONS -- architecture : embed_dim: 1024
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:156] OPTIONS -- DINO
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:158] OPTIONS -- DINO -- loss_weight: 1.0
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:159] OPTIONS -- DINO -- head_n_prototypes: 65536
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:160] OPTIONS -- DINO -- head_bottleneck_dim: 384
I20250304 19:03:59 474390 dinov2 ssl_meta_arch.py:161] OPTIONS -- DINO -- head_hidden_dim: 2048
W20250304 19:03:59 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")

I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:183] OPTIONS -- IBOT
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:184] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:185] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:186] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:194] OPTIONS -- IBOT -- loss_weight: 1.0
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:195] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:196] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20250304 19:04:00 474390 dinov2 ssl_meta_arch.py:197] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:256] Student and Teacher are built: they are both dinov2_vitl14 network.
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:560] DISTRIBUTED FSDP -- preparing model for distributed training
W20250304 19:04:06 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:437: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20250304 19:04:06 474390 dinov2 train.py:382] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (ibot_patch_loss): iBOTPatchLoss()
  (supervised_losses): HierarchicalCrossEntropyLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-23): 24 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=384, bias=True)
        )
        (last_layer): Linear(in_features=384, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (supervised_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): MLP(
        (net): Sequential(
          (0): Linear(in_features=1024, out_features=26, bias=True)
        )
        (last_linear): Linear(in_features=1024, out_features=26, bias=True)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-23): 24 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=384, bias=True)
        )
        (last_layer): Linear(in_features=384, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20250304 19:04:06 474390 dinov2 param_groups.py:54] chunked fsdp
I20250304 19:04:06 474390 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.12.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.13.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.14.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.15.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.16.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.17.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.18.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.19.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.20.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.21.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.22.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] blocks.23.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 19:04:06 474390 dinov2 param_groups.py:64] else code branch
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 19:04:06 474390 dinov2 param_groups.py:64] else code branch
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 19:04:06 474390 dinov2 param_groups.py:64] else code branch
I20250304 19:04:06 474390 dinov2 param_groups.py:87] net.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250304 19:04:06 474390 dinov2 param_groups.py:87] net.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250304 19:04:06 474390 dinov2 ssl_meta_arch.py:547] fusing param groups
I20250304 19:04:07 474390 dinov2 train.py:122] Schedulers ready.
I20250304 19:04:07 474390 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250304 19:04:07 474390 dinov2 augmentations.py:34] ###################################
I20250304 19:04:07 474390 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250304 19:04:07 474390 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250304 19:04:07 474390 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250304 19:04:07 474390 dinov2 augmentations.py:38] local_crops_number: 1
I20250304 19:04:07 474390 dinov2 augmentations.py:39] global_crops_size: 224
I20250304 19:04:07 474390 dinov2 augmentations.py:40] local_crops_size: 98
I20250304 19:04:07 474390 dinov2 augmentations.py:41] ###################################
I20250304 19:04:07 474390 dinov2 augmentations.py:128] ###################################
I20250304 19:04:07 474390 dinov2 augmentations.py:129] Using data augmentation parameters:
I20250304 19:04:07 474390 dinov2 augmentations.py:130] img_size: 224
I20250304 19:04:07 474390 dinov2 augmentations.py:131] ###################################
I20250304 19:04:07 474390 dinov2 loaders.py:93] using dataset: "UnSup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1"
I20250304 19:04:07 474390 dinov2 loaders.py:97] # of dataset samples: 267,888
I20250304 19:04:07 474390 dinov2 loaders.py:93] using dataset: "Sup:df_path=dinobloom_dataset_train_and_test.csv:shuffle=1"
I20250304 19:04:08 474390 dinov2 loaders.py:97] # of dataset samples: 267,888
I20250304 19:04:08 474390 dinov2 loaders.py:120] sampler: infinite
I20250304 19:04:08 474390 dinov2 loaders.py:214] using PyTorch data loader
W20250304 19:04:08 474390 py.warnings warnings.py:109] /home/aih/manon.chossegros/anaconda3/envs/dinobloom2cond/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20250304 19:04:08 474390 dinov2 loaders.py:229] infinite data loader
I20250304 19:04:08 474390 dinov2 loaders.py:155] sampler: distributed
I20250304 19:04:08 474390 dinov2 loaders.py:214] using PyTorch data loader
I20250304 19:04:08 474390 dinov2 loaders.py:227] # of batches: 4,185
I20250304 19:04:08 474390 dinov2 train.py:290] Starting training from iteration 0
